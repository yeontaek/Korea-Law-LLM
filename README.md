

## Update Logs
- 2023.07 ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ [Polyglot-12.8B+RRHF(KR)+LoRa]() ëª¨ë¸ ê³µê°œ ì˜ˆì •
- 2023.07 ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ [Polyglot-12.8B+RRHF(EN)+LoRa]() ëª¨ë¸ ê³µê°œ ì˜ˆì •
- 2023.06 ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ [Law-Polyglot-12.8B+LoRa](https://huggingface.co/yeontaek/Law-polyglot-12.8b-lora) ëª¨ë¸ ê³µê°œ
- 2023.06 ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ [Law-Polyglot-12.8B+QLoRa](https://huggingface.co/yeontaek/Law-polyglot-12.8b-qlora) ëª¨ë¸ ê³µê°œ

---
<br>

# Korea Law Large Language Model

- í•œêµ­ì–´ ë²•ë¥  ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°œë°œí•œ í•œêµ­ì–´ Large Language Model (LLM) 

## Polyglot-ko 12.8B

í•´ë‹¹ ëª¨ë¸ì˜ ëª¨ë“  ì‹¤í—˜ Backbone Modelì€ [Polyglot-ko-12.8B](https://huggingface.co/EleutherAI/polyglot-ko-12.8b) ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ì˜€ë‹¤.

1. Polyglot-ko 12.8B LoRa 8bit -> ğŸ¤— [Law-Polyglot-12.8B LoRa 8bit](https://huggingface.co/yeontaek/Law-polyglot-12.8b-lora)
   - ë°ì´í„°ì…‹ v1: 36,650 ê±´ ë²•ë¥  instruction ë°ì´í„°
2. Polyglot-ko 12.8B QLoRa 4bit -> ğŸ¤— [Law-Polyglot-12.8B QLoRa 4bit](https://huggingface.co/yeontaek/Law-polyglot-12.8b-qlora)
   - ë°ì´í„°ì…‹ v1: 36,650 ê±´ ë²•ë¥  instruction ë°ì´í„°

## ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
### ì§ˆë¬¸: ë²”ì£„ë¥¼ ì €ì§€ë¥´ê³  ì–»ì€ ìˆ˜ì…ì€ ì²´í¬ë˜ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ê±´ê°€ìš”?

### ë‹µë³€: ë¯¸êµ­ì„ ë¹„ë¡¯í•œ ì„¸ê³„ ê°êµ­ì—ì„œëŠ” ìœ ì£„íŒê²°ì„ ë°›ì€ ë²”ì£„ìì˜ ë²”ì£„ìˆ˜ìµì„ ì¶”ì í•´ ëª°ìˆ˜í•˜ëŠ” ì ˆì°¨ê°€ ìˆë‹¤. 
ìš°ë¦¬ë‚˜ë¼ì—ëŠ” 'ë²”ì£„ìˆ˜ìµì€ë‹‰ì˜ ê·œì œ ë° ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥ '(ì´í•˜ë²”ì£„ìˆ˜ìµì€ë‹‰ë²•)ì´ ìˆìœ¼ë©°, ëª°ìˆ˜ëŒ€ìƒ ì¬ì‚°ì—ëŠ” ì˜ˆê¸ˆÂ·ë¶€ë™ì‚° ë“± ì¬ì‚° ë¿ ì•„ë‹ˆë¼ ë²”ì£„ìˆ˜ìµë„ í¬í•¨ë˜ì–´ ìˆë‹¤. 
ë˜í•œ ëª°ìˆ˜ ì™¸ì—ë„ ì¶”ì§•ê¸ˆ, ê³¼ë£Œ(ç§‘æ–™) ë“± ì˜ ëŒ€ì²´ ì§‘í–‰ë°©ë²•ì´ ìˆë‹¤. ë²”ì£„ìˆ˜ìµì€ë‹‰ë²•ì€ ë¯¸êµ­ì˜ ëŒ€í‘œì ì¸ 'ë²Œí¬ìºì‹œë²•(í˜„ê¸ˆì¶”ë°©ë²•)'ì„ ìˆ˜ìš©í•œ ê²ƒìœ¼ë¡œ, ìš°ë¦¬ë‚˜ë¼ì—ë„ ë¯¸êµ­ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì˜ ë²”ì£„ìˆ˜ìµëª°ìˆ˜ë²•ì´ ì ìš©ë˜ê³  ìˆë‹¤.


### ì§ˆë¬¸: ì‹œì •ì•ˆëœ ìë™ì°¨ì— ë¶ˆìƒì˜ ìê°€ ë“¤ì–´ê°€ ìˆëŠ” ê²½ìš°ì—ëŠ” ì–´ë–¤ì²˜ë²Œì´ ê°€ëŠ¥í•˜ë‚˜ìš”?

### ë‹µë³€: ìë™ì°¨ì— ë¶ˆìƒì˜ íƒ€ì¸ì´ ì¹¨ì…í•œ ê²½ìš°ì—ëŠ” ë¶ˆë²•ì£¼ì°¨ì°¨ëŸ‰ì˜ ì°¨ì£¼ëŠ” ë¯¼ë²•ì œ750ì¡°ë¥¼ ê·¼ê±°ë¡œ íƒ€ì¸ì´ ìš´ì „í–ˆë‹¤ê³  ì£¼ì¥í•  ìˆ˜ ìˆìœ¼ë‚˜ ì´ëŠ” ë¶ˆë²•ì£¼ì°¨ì°¨ëŸ‰ì˜ ì°¨ì£¼ì— í•œí•˜ì—¬ ì¸ì •ë˜ëŠ” ê¶Œë¦¬ë¡œì„œ í”¼ê³ ì†Œì¸ì—ê²Œ ì¸ì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ë˜í•œ ìë™ì°¨ì†Œìœ ê¶Œì˜ í¬ê¸°ì˜ì‚¬ë¥¼ ëª…í™•íˆ í‘œëª…í•˜ì§€ ì•ŠëŠ” í•œ íƒ€ì¸ì´ ìš´ì „í•˜ì˜€ë‹¤ê³  í•˜ë”ë¼ë„ ì ˆë„ì£„ëŠ” ì„±ë¦½ë˜ì§€ ì•Šê³  ì ˆë„ì£„ê°€ ì„±ë¦½í•˜ì§€ ì•ŠëŠ” í•œ í”¼ê³ ì†Œì¸ì€ ì ˆë„ì£„ë¡œ ì²˜ë²Œë°›ì§€ ì•Šìœ¼ë©°, ë‹¤ë§Œ ìë™ì°¨ì†í•´ë°°ìƒë³´ì¥ë²•ì œ10ì¡°ì˜3 ì†Œì •ì˜ ì†Œìœ ì ë“±ì˜ ê³ ì˜ì ì¸ ë¶ˆë²•ì£¼ì°¨ë¥¼ ì›ì¸ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°ì—ëŠ” ë¶ˆë²•í–‰ìœ„ë¡œ ì†í•´ë°°ìƒì±…ì„ì´ ë°œìƒí•  ìˆ˜ëŠ” ìˆìŠµë‹ˆë‹¤.
ë”°ë¼ì„œ ê³ ì˜ì ìœ¼ë¡œ ë¶ˆë²•ì£¼ì°¨ëœ ì°¨ëŸ‰ì— ì¹¨ì…í•˜ì—¬ ì¬ë¬¼ì„ ì ˆë„í•œ ê²½ìš°ëŠ” ì ˆë„ì£„ë¡œ ê¸°ì†Œë˜ì–´ ì²˜ë²Œë  ê²ƒì…ë‹ˆë‹¤.
ë‹¤ë§Œ, í”¼ê³ ì†Œì¸ì˜ ê²½ìš° ìë™ì°¨ë“±ë¡ë²ˆí˜¸ê¹Œì§€ ì •í™•íˆ ì•Œê³ ì„œ ìë™ì°¨ë¥¼ ìš´ì „í–ˆë‹¤ê³  í•˜ê¸°ë³´ë‹¤ëŠ” ìë™ì°¨ì†Œìœ ìì˜ ì„±ëª…ë§Œ ì•Œê³ ì„œ ìš´ì „í•œ ê²ƒì´ ëŒ€ë¶€ë¶„ì´ì–´ì„œ ìë™ì°¨ì†Œìœ ìì˜ ì„±ëª…ë§Œ ì•Œê³ ì„œ íƒ€ì¸ì˜ ìë™ì°¨ë¥¼ ìš´ì „í•œ ìëŠ” ë„ë¡œêµí†µë²•ìœ„ë°˜(ì£¼ì •ì°¨ìœ„ë°˜)ì£„ë¡œëŠ” ì²˜ë²Œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

## 1. SFT(supervised-fine-tuning)

### í•™ìŠµ ë°ì´í„° êµ¬ì„±

- ëª¨ë“  SFT í•™ìŠµ ë°ì´í„° êµ¬ì„± ë°©ë²•ì€ [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì˜€ë‹¤.
- Stanford Alpaca í•™ìŠµ ë°ì´í„°ì—ì„œ 40%ëŠ” input ê°’ì„ í• ë‹¹í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ê³  ë‚˜ë¨¸ì§€ 60%ëŠ” inputì„ ë¹ˆê°’ìœ¼ë¡œ í• ë‹¹í•˜ì—¬ êµ¬ì„±
  > input: str, optional context or input for the task. For example, when the instruction is "Summarize the following article", the input is the article. Around 40% of the examples have an input.
  
<br>

```
### Instruction:
{instruction}

### Input:
{input}

### Response:
```

```
### Instruction:
{instruction}

### Response:
```

- ì´ 36,650 ê±´ì˜ instruction ë°ì´í„° ì¤‘ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬, 21,990 ê±´ì— ëŒ€í•´ì„œëŠ” inputì„ ì‚­ì œí•˜ì˜€ìœ¼ë©° ìƒ˜í”Œì€ ì•„ë˜ì™€ ê°™ìŒ
```
{
    "instruction": "ì†ì¸ì£¼ì˜, ì†ì§€ì£¼ì˜ì— ëŒ€í•œ ì •í™•í•œ ì„¤ëª… ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
    "input": "",
    "response": "ì‰½ê²Œ ë§í•´ì„œ, ì†ì¸ì£¼ì´ëŠ” ê·¸ ë‚˜ë¼ì— ì†í•œ ì‚¬ëŒì— ëŒ€í•´ì„œëŠ” ê·¸ ì‚¬ëŒì´ ì™¸êµ­ì— ìˆë”ë¼ë„ ë²•ì´ ì ìš©ëœë‹¤ëŠ” ì´ë¡ ì´ê³ ,ì†ì§€ì£¼ì˜ëŠ” ê·¸ ë‚˜ë¼ì— ìˆëŠ” ì‚¬ëŒì´ë©´ ì™¸êµ­ì¸ì´ë¼ë„ ê·¸ ë‚˜ë¼ì˜ ë²•ì´ ì ìš©ëœë‹¤ëŠ” ì´ë¡ ì…ë‹ˆë‹¤."
}
{
    "instruction": "ë‚´ê¸°ë¡œ í•œ ëˆì„ ì•ˆë‚¸ë‹¤ë©´ ë²•ì ìœ¼ë¡œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‚˜ìš”?",
    "input": "ì§€ì¸ì´ ë³´ì¦ê¸ˆì´ ì—†ì–´ì„œ ì œê°€ ëŒ€ì‹  ì œ ì´ë¦„ìœ¼ë¡œ ì›ë£¸ì„ ê³„ì•½í•´ì£¼ë©° ì›”ì„¸ì™€ ë³µë¹„, ì²­ì†Œë¹„ ë“±ì„ ë‚´ë„ë¡ ì•½ì†ì„ í–ˆìŠµë‹ˆë‹¤. í•œë™ì•ˆì€ ì›”ì„¸ë¥¼ ì˜ ëƒˆëŠ”ë° ê°‘ìê¸° ì›”ì„¸ë¥¼ ë‚´ì§€ì•Šë”ë‹ˆ ì´ëŒ€ë¡œ ê·¸ëƒ¥ ë‚˜ê°€ë²„ë¦¬ê² ë‹¤ë©° ì›”ì„¸ ë³µë¹„ ì²­ì†Œë¹„ ë“± ëª¨ë‘ ë‚´ì§€ì•Šìœ¼ë ¤ê³ í•˜ëŠ”ë° ì œê°€ í•  ìˆ˜ ìˆëŠ”ê±´ ì•„ë¬´ê²ƒë„ ì—†ë‚˜ìš”?",
    "response": "ìƒëŒ€ë°©ì´ ì•½ì •í•œ ë¹„ìš©ì„ ì§€ê¸‰í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì§ˆë¬¸ìë‹˜ì€ ë¯¼ì‚¬ì†Œì†¡ìœ¼ë¡œ ì•½ì •ê¸ˆì— ëŒ€í•œ ì²­êµ¬ì†Œì†¡ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
}
```


### í•™ìŠµ 
- ëª¨ë“  ëª¨ë¸ í•™ìŠµì€ A100 80GB 4ëŒ€ë¡œ í•™ìŠµì„ ì§„í–‰
- í•™ìŠµì— ì‚¬ìš©í•œ ì½”ë“œëŠ” https://github.com/tloen/alpaca-lora,  https://github.com/Beomi/KoAlpaca ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±
- Law-Polyglot-12.8B LoRa 8bit í•™ìŠµ 
<img src="assets/train_lora_8bit.png" width="100%" >

- Law-Polyglot-12.8B QLoRa 4bit í•™ìŠµ 
<img src="assets/train_qlora_4bit.png" width="100%" >



## 2. RRHF(Rank Response from Human Feedback)
- [paper](https://arxiv.org/pdf/2304.05302v1.pdf)ì—ì„œ ì œì•ˆëœ RRHF ë°©ë²•ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ ë° í‰ê°€ë¥¼ ì§„í–‰í•˜ê³ ì í•œë‹¤.
- í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” RLHFì˜ ê°•í™”í•™ìŠµ ëŒ€ì‹ ì— RRHF(Rank Response from Human Feedback)ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ëŠ”ë° ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤.
  - ì‚¬ì „ì— datasetì„ ë§Œë“¤ì–´ì„œ Supervised Learningìœ¼ë¡œ ì ‘ê·¼í•˜ëŠ”ë°. ê° query ëŒ€í•´ ì—¬ëŸ¬ Responseë¥¼ ìƒì„±í•˜ê³  í•´ë‹¹ Responseë“¤ì˜ ì ìˆ˜ë¥¼ ì¸¡ì •
  - SFT Loss : ê° ì§ˆì˜ì˜ ì‘ë‹µ ì¤‘ ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ best responseì— ëŒ€í•´ì„œëŠ” SFT Lossë¥¼ ì¸¡ì • 
  - Rank Loss : ê° ì§ˆì˜ì˜ ì‘ë‹µ ì¤‘ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìˆœìœ„ì™€ í•™ìŠµ ë°ì´í„°ì— ìˆëŠ” ì •ë‹µ ìˆœìœ„ë¥¼ ê°€ì§€ê³  Rank Lossë¥¼ ì¸¡ì • 
  - ìµœì¢… LossëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±ëœë‹¤. 
    > loss = rrhf_weight * rrhf_loss + sft_loss


### í•™ìŠµ ë°ì´í„° êµ¬ì„±
- ì €ìê°€ ê³µê°œí•œ ì˜ì–´ í•™ìŠµ ë°ì´í„° ì•½ 7ë§Œ ê±´ì„ ê°€ì§€ê³  ìš°ì„  í•™ìŠµì„ ì§„í–‰, [ë§í¬](https://drive.google.com/file/d/1nAfBt0ldSy7m5O-Sgt05SQ1rK__NmC2Z/view?usp=sharing)
- ë°ì´í„° êµ¬ì„±ì€ í•˜ë‚˜ì˜ queryì™€ 6ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ responsesìœ¼ë¡œ êµ¬ì„± ë˜ì–´ìˆìœ¼ë©° ì ìˆ˜ë„ í• ë‹¹ë˜ì–´ ìˆë‹¤. 

```
{
    "query": " \n\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\n\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you\u2019ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\n\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\n\nAssistant:", 
    "responses": [
    " You can, but it's not necessary. The microfiber cloth should be effective enough on its own.", 
    " No, that is not recommended. Isopropyl alcohol can damage the screen and should not be used for cleaning.", 
    " You can, but it's not necessary. The microfiber cloth should be effective enough on its own. Just make sure to use gentle, circular motions when cleaning the screen.", 
    " No, that is not recommended. Isopropyl alcohol can damage the screen and should not be used for cleaning. You should use a clean cloth and water instead.", 
    " Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it\u2019s lint-free.", 
    " Yes, you can spray it directly onto the cloth."], 
    "scores": [-0.8408203125, -0.85400390625, -0.57421875, -0.62841796875, -0.492431640625, -0.8798828125]
}
```

### í•™ìŠµ 
- í•œêµ­ì–´ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë–„ë¬¸ì— ì €ì ê³µê°œí•œ í•™ìŠµ ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ Polyglot-ko 12.8B ëª¨ë¸ì— í•™ìŠµì„ ì§„í–‰ 
- ì €ìê°€ ê³µê°œí•œ í•™ìŠµ ì½”ë“œëŠ” full-finetuningìœ¼ë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ loraì™€ qloraê°€ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì¬êµ¬ì„±í•˜ì˜€ë‹¤.
- ë˜í•œ Polyglot-ko 12.8B ëª¨ë¸ì— ë§ê²Œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ë³€ê²½í•˜ì˜€ìŒ




## 3. TO-DO
### ëª¨ë¸ í‰ê°€ 

### RRHF í•œêµ­ì–´ í•™ìŠµ ë°ì´í„° êµ¬ì¶•


## 4. Reference
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [Alpaca LoRa](https://github.com/tloen/alpaca-lora)
- [KoAlpaca](https://github.com/Beomi/KoAlpaca)
- [EleutherAI](https://github.com/EleutherAI)
- [Wombat ğŸ»â€â„ï¸: from RLHF to RRHF](https://github.com/GanjinZero/RRHF)
- [RRHF: Rank Responses to Align Language Models with Human Feedback without tears](https://arxiv.org/pdf/2304.05302v1.pdf)